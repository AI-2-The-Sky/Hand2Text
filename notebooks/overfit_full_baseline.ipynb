{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "from nptyping import Float32, NDArray, Number, Shape, UInt\n",
    "from transformers import ViTModel\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignedDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_FeatureExtractor(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# print(\"---VIT INIT---\")\n",
    "\n",
    "\t\tself.pretrained_vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\t\tself.pretrained_vit.eval()\n",
    "\n",
    "\t\tself.conv_1d_1 = torch.nn.Conv1d(\n",
    "\t\t\tin_channels=197,\n",
    "\t\t\tout_channels=64,\n",
    "\t\t\tkernel_size=3,\n",
    "\t\t)\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\tself.conv_1d_2 = torch.nn.Conv1d(\n",
    "\t\t\tin_channels=64,\n",
    "\t\t\tout_channels=nb_classes, # <-- i/o 1\n",
    "\t\t\tkernel_size=3,\n",
    "\t\t)\n",
    "\t\tself.layer_2_relu = nn.ReLU()\n",
    "\n",
    "\tdef vit_extract_features(self, x):\n",
    "\t\t# print(\"---VIT EXTRACT FEATURES---\")\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = self.pretrained_vit(pixel_values=x)\n",
    "\t\t\tvit_feat = outputs.last_hidden_state\n",
    "\t\t\tprint(f\"{vit_feat.shape= }\")\n",
    "\t\t\tvit_feat = torch.flatten(vit_feat, start_dim=1)\n",
    "\t\treturn vit_feat\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tvit_feat, \n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\t# print(\"---VIT FORWARD---\")\n",
    "\t\tx = self.conv_1d_1(vit_feat)\n",
    "\t\tx = self.layer_1_relu(x)\n",
    "\t\tx = self.conv_1d_2(x)\n",
    "\t\tx = self.layer_2_relu(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Translator(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes,\n",
    "\t\tH_input_size: int = 151296,\n",
    "\t\tH_output_size: int = 10,\n",
    "\t\tnum_layers: int = 1,\n",
    "\t\tdropout: int = 0,\n",
    "\t):\n",
    "\t\t# print(\"---GRU INIT---\")\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\t\tself.vocabulary_size = nb_classes\n",
    "\t\tself.layer_gru = nn.GRU(\n",
    "\t\t\tinput_size=self.hparams.H_input_size,\n",
    "\t\t\thidden_size=self.hparams.nb_classes,\n",
    "\t\t\tnum_layers=self.hparams.num_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=self.hparams.dropout,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.layer_1_dense = nn.Linear(self.hparams.nb_classes, self.hparams.nb_classes)\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\t# self.layer_2_dense = nn.Linear(self.hparams.H_output_size, self.vocabulary_size)\n",
    "\t\t# self.layer_2_relu = nn.ReLU()\n",
    "\t\tself.softmax = nn.Softmax(dim=2) # <-- i/o dim=2\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# print(\"---GRU FORWARD---\")\t\n",
    "\t\t# print(f'{X.shape =}')\n",
    "\t\tX, _ = self.layer_gru(X)\n",
    "\t\tX = self.layer_1_dense(X)\n",
    "\t\t# X = self.layer_1_relu(X)\n",
    "\t\t# X = self.layer_2_dense(X)\n",
    "\t\t# X = self.layer_2_relu(X)\n",
    "\t\t# print(f'{X.shape =}')\n",
    "\t\t# print(f\"avant softmax: {X=}\")\n",
    "\t\tX = self.softmax(X)\n",
    "\t\t# print(f\"apres softmax: {X=}\")\n",
    "\t\t# print(f'{X.shape =}')\n",
    "\t\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseSquareNet(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tbatch_size: int = 1,\n",
    "\t\tseq_size: int = 1,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t\th_in: int = 10,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.nb_seq_sizebatch = seq_size\n",
    "\t\tself.image_feature_extractr = ViT_FeatureExtractor(nb_classes=nb_classes)\n",
    "\t\tself.recurrent_translator = GRU_Translator(\n",
    "\t\t\tnb_classes = nb_classes,\n",
    "\t\t\tH_input_size=h_in,\n",
    "\t\t\t# H_output_size=100,\n",
    "\t\t\tnum_layers=1,\n",
    "\t\t\tdropout=0,\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\tx = self.recurrent_translator(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSquareNet(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tbatch_size: int = 1,\n",
    "\t\tseq_size: int = 1,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t\th_in: int = 10,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.nb_seq_sizebatch = seq_size\n",
    "\t\tself.image_feature_extractr = ViT_FeatureExtractor(nb_classes=nb_classes)\n",
    "\t\tself.recurrent_translator = GRU_Translator(\n",
    "\t\t\tnb_classes = nb_classes,\n",
    "\t\t\tH_input_size=h_in,\n",
    "\t\t\t# H_output_size=100,\n",
    "\t\t\tnum_layers=1,\n",
    "\t\t\tdropout=0,\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\tx = self.recurrent_translator(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nb_classes=10\n",
    "seq_size = 2\n",
    "batch_size = 2\n",
    "learning_rate = 1e-4\n",
    "h_in = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[[0.1326, 0.8331, 0.4766, 0.6109, 0.9087, 0.2746, 0.8763, 0.0457,\n",
      "          0.7431, 0.3337],\n",
      "         [0.7922, 0.1349, 0.3717, 0.6364, 0.8544, 0.5529, 0.1504, 0.8544,\n",
      "          0.5674, 0.2392],\n",
      "         [0.5797, 0.9653, 0.5151, 0.7264, 0.9557, 0.0348, 0.6179, 0.0335,\n",
      "          0.8419, 0.4402],\n",
      "         [0.6365, 0.3544, 0.1200, 0.0209, 0.3555, 0.0998, 0.2111, 0.8125,\n",
      "          0.6757, 0.2069]],\n",
      "\n",
      "        [[0.8706, 0.1550, 0.1041, 0.6045, 0.5984, 0.1453, 0.5757, 0.9145,\n",
      "          0.1843, 0.6810],\n",
      "         [0.8202, 0.2809, 0.8687, 0.7622, 0.7350, 0.3684, 0.2455, 0.5685,\n",
      "          0.3301, 0.8901],\n",
      "         [0.1646, 0.9842, 0.0604, 0.3706, 0.1392, 0.7003, 0.4133, 0.4687,\n",
      "          0.8430, 0.7095],\n",
      "         [0.2424, 0.4553, 0.8838, 0.0189, 0.9084, 0.4076, 0.3309, 0.2160,\n",
      "          0.4893, 0.4794]],\n",
      "\n",
      "        [[0.2542, 0.6651, 0.3881, 0.3942, 0.7424, 0.0986, 0.8370, 0.2860,\n",
      "          0.8439, 0.7679],\n",
      "         [0.9598, 0.7701, 0.2160, 0.3785, 0.6271, 0.7881, 0.5095, 0.0943,\n",
      "          0.5708, 0.7042],\n",
      "         [0.9080, 0.9531, 0.6351, 0.6404, 0.4422, 0.0310, 0.2426, 0.8636,\n",
      "          0.1294, 0.7836],\n",
      "         [0.6707, 0.5234, 0.6530, 0.2539, 0.9156, 0.2540, 0.8009, 0.0240,\n",
      "          0.6918, 0.9593]],\n",
      "\n",
      "        [[0.0670, 0.6274, 0.2477, 0.1179, 0.4639, 0.9552, 0.1288, 0.1438,\n",
      "          0.8443, 0.6403],\n",
      "         [0.9836, 0.0061, 0.3732, 0.0380, 0.5665, 0.9885, 0.6569, 0.9732,\n",
      "          0.9126, 0.4226],\n",
      "         [0.1168, 0.9941, 0.5437, 0.0078, 0.9050, 0.3504, 0.1211, 0.3649,\n",
      "          0.2528, 0.8914],\n",
      "         [0.5302, 0.8134, 0.5482, 0.1115, 0.8615, 0.3999, 0.5718, 0.7909,\n",
      "          0.0255, 0.9894]],\n",
      "\n",
      "        [[0.4619, 0.4777, 0.5711, 0.7023, 0.6250, 0.9705, 0.8071, 0.4256,\n",
      "          0.3055, 0.8144],\n",
      "         [0.5570, 0.8626, 0.9081, 0.0419, 0.1312, 0.2736, 0.3783, 0.0061,\n",
      "          0.2626, 0.7733],\n",
      "         [0.1145, 0.6391, 0.0201, 0.3338, 0.8872, 0.4527, 0.6924, 0.3627,\n",
      "          0.8459, 0.5677],\n",
      "         [0.8810, 0.3852, 0.7574, 0.7729, 0.5578, 0.2313, 0.5485, 0.2719,\n",
      "          0.2340, 0.9944]],\n",
      "\n",
      "        [[0.2707, 0.8051, 0.0746, 0.3586, 0.2445, 0.0831, 0.9592, 0.0214,\n",
      "          0.9496, 0.4590],\n",
      "         [0.2951, 0.5245, 0.2540, 0.7118, 0.9502, 0.7781, 0.5749, 0.8875,\n",
      "          0.6832, 0.1783],\n",
      "         [0.9564, 0.7535, 0.5143, 0.3525, 0.2536, 0.6544, 0.1480, 0.2399,\n",
      "          0.2300, 0.1392],\n",
      "         [0.9197, 0.1654, 0.9416, 0.5994, 0.2284, 0.9326, 0.9432, 0.7663,\n",
      "          0.6722, 0.4222]],\n",
      "\n",
      "        [[0.4604, 0.6067, 0.4408, 0.3843, 0.4691, 0.9326, 0.6261, 0.6796,\n",
      "          0.7207, 0.4456],\n",
      "         [0.3438, 0.6190, 0.5377, 0.2071, 0.9456, 0.1272, 0.0286, 0.8121,\n",
      "          0.1179, 0.5208],\n",
      "         [0.5109, 0.7617, 0.8144, 0.7713, 0.3710, 0.1346, 0.9097, 0.4583,\n",
      "          0.9571, 0.6999],\n",
      "         [0.6529, 0.5632, 0.7045, 0.9625, 0.9586, 0.0084, 0.7658, 0.2991,\n",
      "          0.3983, 0.3000]],\n",
      "\n",
      "        [[0.3216, 0.4801, 0.2774, 0.1709, 0.9021, 0.5305, 0.5812, 0.2013,\n",
      "          0.0321, 0.2076],\n",
      "         [0.9412, 0.6979, 0.7576, 0.2068, 0.7524, 0.8129, 0.7088, 0.5531,\n",
      "          0.7233, 0.7350],\n",
      "         [0.7598, 0.6344, 0.1733, 0.9672, 0.7099, 0.4901, 0.7107, 0.6391,\n",
      "          0.1599, 0.0761],\n",
      "         [0.9006, 0.7947, 0.6915, 0.1433, 0.1163, 0.1916, 0.4440, 0.1959,\n",
      "          0.9003, 0.8492]]])\n",
      "y = tensor([[1406, 1107,  753,  773],\n",
      "        [1227, 1699, 1376,   65],\n",
      "        [ 339, 1406, 1812, 1919],\n",
      "        [ 571, 1597,  260,  739],\n",
      "        [1430, 1130,  621, 1871],\n",
      "        [ 324,  662,  314,  170],\n",
      "        [1788, 1501,  884, 1222],\n",
      "        [1794, 1528,  973, 1585]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((batch_size * seq_size, 3, 224, 224))\n",
    "y = torch.randint(0, nb_classes, (batch_size, seq_size))\n",
    "\n",
    "# print(f\"{y.size()=}\")\n",
    "# print(f\"{y=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# Models\n",
    "# vit_feat = model.image_feature_extractr.vit_extract_features(x)\n",
    "\n",
    "# dataset = SignedDataset(vit_feat, y)\n",
    "model = BaseSquareNet(batch_size=batch_size, seq_size=seq_size, nb_classes=nb_classes, h_in=h_in)\n",
    "vit_feat = model.image_feature_extractr.vit_extract_features(x)\n",
    "dataset = SignedDataset(vit_feat, y)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\t# lr = 10 ** (- e_lr / 10)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader, model, loss_fn, learning_rate, epochs=100):\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\tlosses = []\n",
    "\tidx = 0\n",
    "\tfor epoch in tqdm(range(epochs)):\n",
    "\t\tfor batch_idx, (X, y) in enumerate(train_loader):\n",
    "\t\t\tpred = model(X)\n",
    "\t\t\tpred = pred.permute(0, 2, 1)\n",
    "\t\t\tloss = loss_fn(pred, y)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# if epoch % 10 == 0:\n",
    "\t\t\t# \tprint(f'[{epoch = }] loss: {loss}')\n",
    "\t\t\tidx += 1\n",
    "\t\t\tlosses.append(float(loss.detach().numpy()))\n",
    "\treturn losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 247/1000 [01:06<02:40,  4.70it/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "torch.manual_seed(6)\n",
    "\n",
    "for lr in [1e-3]:\n",
    "\t# lr = 10 ** (- e_lr / 10)\n",
    "\t# model = GRU(input_size=h_in, hidden_size=5, output_size=nb_classes, num_layers=1, bias=0)\n",
    "\tmodel = BaseSquareNet(nb_classes=nb_classes, seq_size=seq_size, batch_size=batch_size, h_in=h_in)\n",
    "\tlosses = train(dataloader, model, loss_fn, learning_rate=lr, epochs=int(1e3))\n",
    "\tprint(f\"For {lr = }, {min(losses) = }\")\n",
    "\tplt.plot(losses, label=f\"{lr:e}\")\n",
    "leg = plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Hand2Text')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65fb5dd8ae324f83aa5144c1c5dc9ef961af4827b21b9b1b28ba5f6a989edf83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
