{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nptyping in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from nptyping) (1.22.3)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.0.0 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from nptyping) (4.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nptyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (4.20.1)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (1.22.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/miniconda3/envs/Hand2Text/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "from nptyping import Float32, NDArray, Number, Shape, UInt\n",
    "from transformers import ViTModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignedDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_FeatureExtractor(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# print(\"---VIT INIT---\")\n",
    "\n",
    "\t\tself.pretrained_vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\t\tself.pretrained_vit.eval()\n",
    "\n",
    "\t\tself.conv_1d_1 = torch.nn.Conv1d(\n",
    "\t\t\tin_channels=197,\n",
    "\t\t\tout_channels=64,\n",
    "\t\t\tkernel_size=3,\n",
    "\t\t)\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\tself.conv_1d_2 = torch.nn.Conv1d(\n",
    "\t\t\tin_channels=64,\n",
    "\t\t\tout_channels=nb_classes, # <-- i/o 1\n",
    "\t\t\tkernel_size=3,\n",
    "\t\t)\n",
    "\t\tself.layer_2_relu = nn.ReLU()\n",
    "\n",
    "\tdef vit_extract_features(self, x):\n",
    "\t\t# print(\"---VIT EXTRACT FEATURES---\")\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = self.pretrained_vit(pixel_values=x)\n",
    "\t\t\tvit_feat = outputs.last_hidden_state\n",
    "\t\t\tprint(f\"{vit_feat.shape= }\")\n",
    "\t\t\tvit_feat = torch.flatten(vit_feat, start_dim=1)\n",
    "\t\treturn vit_feat\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tvit_feat, \n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\t# print(\"---VIT FORWARD---\")\n",
    "\t\tx = self.conv_1d_1(vit_feat)\n",
    "\t\tx = self.layer_1_relu(x)\n",
    "\t\tx = self.conv_1d_2(x)\n",
    "\t\tx = self.layer_2_relu(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t):\n",
    "\t\t# print(\"---BASIC MODEL INIT---\")\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.vocabulary_size = nb_classes\n",
    "\t\tself.layer = nn.Linear(151296, self.vocabulary_size)\n",
    "\t\tself.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\t# print(\"---BASIC MODEL FORWARD---\")\n",
    "\t\tx = self.layer(x)\n",
    "\t\tx = self.softmax(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Translator(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes,\n",
    "\t\tH_input_size: int = 151296,\n",
    "\t\tH_output_size: int = 100,\n",
    "\t\tnum_layers: int = 1,\n",
    "\t\tdropout: int = 0,\n",
    "\t):\n",
    "\t\t# print(\"---GRU INIT---\")\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\t\tself.vocabulary_size = nb_classes\n",
    "\t\tself.layer_gru = nn.GRU(\n",
    "\t\t\tinput_size=self.hparams.H_input_size,\n",
    "\t\t\thidden_size=self.hparams.H_output_size,\n",
    "\t\t\tnum_layers=self.hparams.num_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=self.hparams.dropout,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.layer_1_dense = nn.Linear(self.hparams.H_output_size, self.hparams.H_output_size)\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\tself.layer_2_dense = nn.Linear(self.hparams.H_output_size, self.vocabulary_size)\n",
    "\t\tself.layer_2_relu = nn.ReLU()\n",
    "\t\tself.softmax = nn.Softmax(dim=1) # <-- i/o dim=2\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# print(\"---GRU FORWARD---\")\t\n",
    "\t\tX, _ = self.layer_gru(X)\n",
    "\t\tX = self.layer_1_dense(X)\n",
    "\t\tX = self.layer_1_relu(X)\n",
    "\t\tX = self.layer_2_dense(X)\n",
    "\t\tX = self.layer_2_relu(X)\n",
    "\t\tX = self.softmax(X)\n",
    "\t\treturn X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseSquareNet(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tbatch_size: int = 1,\n",
    "\t\tseq_size: int = 1,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.nb_seq_sizebatch = seq_size\n",
    "\t\tself.image_feature_extractr = ViT_FeatureExtractor(nb_classes=nb_classes)\n",
    "\t\tself.recurrent_translator = GRU_Translator(\n",
    "\t\t\tnb_classes = nb_classes,\n",
    "\t\t\tH_input_size=151296,\n",
    "\t\t\tH_output_size=100,\n",
    "\t\t\tnum_layers=1,\n",
    "\t\t\tdropout=0,\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\tx = self.recurrent_translator(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nb_classes=1999\n",
    "seq_size = 16\n",
    "batch_size = 1\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.size()=torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "# x = torch.rand((batch_size, seq_size, 3, 224, 224))\n",
    "# y = torch.randint(0, nb_classes, (batch_size, seq_size, 1))\n",
    "\n",
    "x = torch.rand((batch_size, 3, 224, 224))\n",
    "y = torch.randint(0, nb_classes, (batch_size,))\n",
    "\n",
    "print(f\"{y.size()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_feat.shape= torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "model = BaseSquareNet(nb_classes=nb_classes, seq_size=seq_size, batch_size=batch_size)\n",
    "vit_feat = model.image_feature_extractr.vit_extract_features(x)\n",
    "\n",
    "dataset = SignedDataset(vit_feat, y)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train(train_loader, model, loss_fn, optimizer):\n",
    "\tloss = 10\n",
    "\tidx = 0\n",
    "\twhile loss > 6.7:\n",
    "\t\tfor batch_idx, (X, y) in enumerate(train_loader):\n",
    "\t\t\tpred = model(X)\n",
    "\t\t\tloss = loss_fn(pred, y)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\tif batch_idx % 10 == 0:\n",
    "\t\t\t\tprint(f'[{idx}] loss: {loss}\\r', end='')\n",
    "\t\t\tidx += 1\n",
    "\n",
    "\tprint(f'[{idx}] final loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] final loss: 6.627474308013916\n"
     ]
    }
   ],
   "source": [
    "train(dataloader, model, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e5cf168e483d0a9ac2f5326c7238cfe4405c28e0dcf7dcfceeeb179bf0db248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
