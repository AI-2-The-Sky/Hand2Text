{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nptyping in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (2.2.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.0.0 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from nptyping) (4.3.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from nptyping) (1.23.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install nptyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (4.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (1.23.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: filelock in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "from nptyping import Float32, NDArray, Number, Shape, UInt\n",
    "from transformers import ViTModel\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignedDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_FeatureExtractor(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# print(\"---VIT INIT---\")\n",
    "\n",
    "\t\tself.pretrained_vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\t\tself.pretrained_vit.eval()\n",
    "\n",
    "\t\tself.conv_1d_1 = torch.nn.Conv1d(\n",
    "\t\t\tin_channels=197,\n",
    "\t\t\tout_channels=64,\n",
    "\t\t\tkernel_size=3,\n",
    "\t\t)\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\tself.conv_1d_2 = torch.nn.Conv1d(\n",
    "\t\t\tin_channels=64,\n",
    "\t\t\tout_channels=nb_classes, # <-- i/o 1\n",
    "\t\t\tkernel_size=3,\n",
    "\t\t)\n",
    "\t\tself.layer_2_relu = nn.ReLU()\n",
    "\n",
    "\tdef vit_extract_features(self, x):\n",
    "\t\t# print(\"---VIT EXTRACT FEATURES---\")\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = self.pretrained_vit(pixel_values=x)\n",
    "\t\t\tvit_feat = outputs.last_hidden_state\n",
    "\t\t\tprint(f\"{vit_feat.shape= }\")\n",
    "\t\t\tvit_feat = torch.flatten(vit_feat, start_dim=1)\n",
    "\t\treturn vit_feat\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tvit_feat, \n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\t# print(\"---VIT FORWARD---\")\n",
    "\t\tx = self.conv_1d_1(vit_feat)\n",
    "\t\tx = self.layer_1_relu(x)\n",
    "\t\tx = self.conv_1d_2(x)\n",
    "\t\tx = self.layer_2_relu(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t):\n",
    "\t\t# print(\"---BASIC MODEL INIT---\")\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.vocabulary_size = nb_classes\n",
    "\t\tself.layer = nn.Linear(151296, self.vocabulary_size)\n",
    "\t\tself.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\t# print(\"---BASIC MODEL FORWARD---\")\n",
    "\t\tx = self.layer(x)\n",
    "\t\tx = self.softmax(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Translator(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes,\n",
    "\t\tH_input_size: int = 151296,\n",
    "\t\tH_output_size: int = 10,\n",
    "\t\tnum_layers: int = 1,\n",
    "\t\tdropout: int = 0,\n",
    "\t):\n",
    "\t\t# print(\"---GRU INIT---\")\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\t\tself.vocabulary_size = nb_classes\n",
    "\t\tself.layer_gru = nn.GRU(\n",
    "\t\t\tinput_size=self.hparams.H_input_size,\n",
    "\t\t\thidden_size=self.hparams.nb_classes,\n",
    "\t\t\tnum_layers=self.hparams.num_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=self.hparams.dropout,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# self.layer_1_dense = nn.Linear(self.hparams.H_output_size, self.hparams.H_output_size)\n",
    "\t\t# self.layer_1_relu = nn.ReLU()\n",
    "\t\t# self.layer_2_dense = nn.Linear(self.hparams.H_output_size, self.vocabulary_size)\n",
    "\t\t# self.layer_2_relu = nn.ReLU()\n",
    "\t\tself.softmax = nn.Softmax(dim=2) # <-- i/o dim=2\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\t# print(\"---GRU FORWARD---\")\t\n",
    "\t\t# print(f'{X.shape =}')\n",
    "\t\tX, _ = self.layer_gru(X)\n",
    "\t\t# X = self.layer_1_dense(X)\n",
    "\t\t# X = self.layer_1_relu(X)\n",
    "\t\t# X = self.layer_2_dense(X)\n",
    "\t\t# X = self.layer_2_relu(X)\n",
    "\t\t# print(f'{X.shape =}')\n",
    "\t\tX = self.softmax(X)\n",
    "\t\t# print(f'{X.shape =}')\n",
    "\t\treturn X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseSquareNet(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tbatch_size: int = 1,\n",
    "\t\tseq_size: int = 1,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t\th_in: int = 10,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.nb_seq_sizebatch = seq_size\n",
    "\t\tself.image_feature_extractr = ViT_FeatureExtractor(nb_classes=nb_classes)\n",
    "\t\tself.recurrent_translator = GRU_Translator(\n",
    "\t\t\tnb_classes = nb_classes,\n",
    "\t\t\tH_input_size=h_in,\n",
    "\t\t\t# H_output_size=100,\n",
    "\t\t\tnum_layers=1,\n",
    "\t\t\tdropout=0,\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\tx = self.recurrent_translator(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nb_classes=4\n",
    "seq_size = 3\n",
    "batch_size = 2\n",
    "learning_rate = 1e-2\n",
    "h_in = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[[0.0847, 0.3212, 0.4757, 0.4554],\n",
      "         [0.0679, 0.1502, 0.5550, 0.3498],\n",
      "         [0.7801, 0.4790, 0.9360, 0.8824]],\n",
      "\n",
      "        [[0.9988, 0.6722, 0.2174, 0.8381],\n",
      "         [0.9823, 0.3689, 0.7894, 0.2477],\n",
      "         [0.7110, 0.1337, 0.4323, 0.0889]]])\n",
      "y = tensor([[0, 3, 3],\n",
      "        [1, 1, 3]])\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "# x = torch.rand((batch_size, seq_size, 3, 224, 224))\n",
    "# y = torch.randint(0, nb_classes, (batch_size, seq_size, 1))\n",
    "\n",
    "# x = torch.rand((batch_size, 3, 224, 224))\n",
    "x = torch.rand((batch_size, seq_size, h_in))\n",
    "y = torch.randint(0, nb_classes, (batch_size, seq_size))\n",
    "\n",
    "print(f\"{x = }\")\n",
    "print(f\"{y = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRUCell(nn.Module):\n",
    "\n",
    "#     \"\"\"\n",
    "#     An implementation of GRUCell.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, input_size, hidden_size, bias=True):\n",
    "#         super(GRUCell, self).__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.bias = bias\n",
    "#         self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
    "#         self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
    "#         self.reset_parameters()\n",
    "\n",
    "\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         std = 1.0 / math.sqrt(self.hidden_size)\n",
    "#         for w in self.parameters():\n",
    "#             w.data.uniform_(-std, std)\n",
    "    \n",
    "#     def forward(self, x, hidden):\n",
    "        \n",
    "#         x = x.view(-1, x.size(1))\n",
    "        \n",
    "#         gate_x = self.x2h(x) \n",
    "#         gate_h = self.h2h(hidden)\n",
    "        \n",
    "#         gate_x = gate_x.squeeze()\n",
    "#         gate_h = gate_h.squeeze()\n",
    "        \n",
    "#         i_r, i_i, i_n = gate_x.chunk(3, 1)\n",
    "#         h_r, h_i, h_n = gate_h.chunk(3, 1)\n",
    "        \n",
    "        \n",
    "#         resetgate = F.sigmoid(i_r + h_r)\n",
    "#         inputgate = F.sigmoid(i_i + h_i)\n",
    "#         newgate = F.tanh(i_n + (resetgate * h_n))\n",
    "        \n",
    "#         hy = newgate + inputgate * (hidden - newgate)\n",
    "        \n",
    "        \n",
    "#         return hy\n",
    "\n",
    "# class GRUModel(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):\n",
    "#         super(GRUModel, self).__init__()\n",
    "#         # Hidden dimensions\n",
    "#         self.hidden_dim = hidden_dim\n",
    "         \n",
    "#         # Number of hidden layers\n",
    "#         self.layer_dim = layer_dim\n",
    "         \n",
    "       \n",
    "#         self.gru_cell = GRUCell(input_dim, hidden_dim, layer_dim)\n",
    "        \n",
    "        \n",
    "#         self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "     \n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         # Initialize hidden state with zeros\n",
    "#         #######################\n",
    "#         #  USE GPU FOR MODEL  #\n",
    "#         #######################\n",
    "#         #print(x.shape,\"x.shape\")100, 28, 28\n",
    "#         if torch.cuda.is_available():\n",
    "#             h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "#         else:\n",
    "#             h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "         \n",
    "       \n",
    "#         outs = []\n",
    "        \n",
    "#         hn = h0[0,:,:]\n",
    "        \n",
    "#         for seq in range(x.size(1)):\n",
    "#             hn = self.gru_cell(x[:,seq,:], hn) \n",
    "#             outs.append(hn)\n",
    "            \n",
    "#         out = outs[-1].squeeze()\n",
    "        \n",
    "#         out = self.fc(out) \n",
    "#         # out.size() --> 100, 10\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# Models\n",
    "# vit_feat = model.image_feature_extractr.vit_extract_features(x)\n",
    "\n",
    "# dataset = SignedDataset(vit_feat, y)\n",
    "dataset = SignedDataset(x, y)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "\t# lr = 10 ** (- e_lr / 10)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def train(train_loader, model, loss_fn, optimizer, nb_epoch):\n",
    "\tloss = 10\n",
    "\tidx = 0\n",
    "\tloss_array = []\n",
    "\twhile idx < nb_epoch:\n",
    "\t\tfor batch_idx, (X, y) in enumerate(train_loader):\n",
    "\t\t\tpred = model(X)\n",
    "\t\t\tpred = pred.permute(0, 2, 1)\n",
    "\t\t\tloss = loss_fn(pred, y)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\tloss_array.append(float(loss))\n",
    "\t\t\tif idx % 10 == 0:\n",
    "\t\t\t\tprint(f'[{idx}] loss: {loss}')\n",
    "\t\t\tidx += 1\n",
    "\n",
    "\tprint(f'[{idx}] final loss: {loss}')\n",
    "\treturn loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] loss: 7.600404739379883\n",
      "[10] loss: 7.600404739379883\n",
      "[20] loss: 7.600404739379883\n",
      "[30] loss: 7.600404739379883\n",
      "[40] loss: 7.600404739379883\n",
      "[50] final loss: 7.600404739379883\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_array = train(dataloader, model, loss_fn, optimizer, 50)\n",
    "plt.plot(loss_array)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
<<<<<<< HEAD
   "display_name": "Python 3.8.13 ('Hand2Text')",
=======
   "display_name": "Python 3.10.4 64-bit",
>>>>>>> 32ec34b ( e)
=======
   "display_name": "Python 3.8.13 ('Hand2Text')",
>>>>>>> 85f0ffc (save)
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
<<<<<<< HEAD
<<<<<<< HEAD
    "hash": "cf687d64d53e3adc62f479116506dad1cadccbe1000f1194a9eb47c325d46594"
=======
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
>>>>>>> 32ec34b ( e)
=======
    "hash": "65fb5dd8ae324f83aa5144c1c5dc9ef961af4827b21b9b1b28ba5f6a989edf83"
>>>>>>> 85f0ffc (save)
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
