{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "from nptyping import Float32, NDArray, Number, Shape, UInt\n",
    "from transformers import ViTModel\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignedDataset(Dataset):\n",
    "    def __init__(self, X, Y, seq_size):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.seq_size = seq_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x_idx_a = seq_size *  i\n",
    "        x_idx_z = seq_size * (i+1)\n",
    "        batch_X = self.X[x_idx_a:x_idx_z]\n",
    "\t\t\n",
    "        batch_Y = self.Y[i]\n",
    "        return batch_X, batch_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_FeatureExtractor(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t\tbatch_size: int = 2,\n",
    "\t\tseq_size: int = 2,\n",
    "\t\tout_features: int = 64,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.seq_size = seq_size\n",
    "\t\tself.pretrained_vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\t\tself.pretrained_vit.eval()\n",
    "\t\tself.conv_1d_1 = torch.nn.Conv1d(\n",
    "\t\t\tin_channels=197,\n",
    "\t\t\tout_channels=self.hparams.out_features,\n",
    "\t\t\tkernel_size=768,\n",
    "\t\t)\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\t# self.conv_1d_2 = torch.nn.Conv1d(\n",
    "\t\t# \tin_channels=64,\n",
    "\t\t# \tout_channels=nb_classes, # <-- i/o 1\n",
    "\t\t# \tkernel_size=3,\n",
    "\t\t# )\n",
    "\t\t# self.layer_2_relu = nn.ReLU()\n",
    "\n",
    "\tdef vit_extract_features(self, x):\n",
    "\t\t# print(\"---VIT EXTRACT FEATURES---\")\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = self.pretrained_vit(pixel_values=x)\n",
    "\t\t\tvit_feat = outputs.last_hidden_state\n",
    "\t\t\tprint(f\"{vit_feat.shape = }\")\n",
    "\t\t\t# vit_feat = torch.flatten(vit_feat, start_dim=1)\n",
    "\t\t\t# b, f = vit_feat.size()\n",
    "\t\t\t# vit_feat = torch.reshape(vit_feat, (self.batch_size, self.seq_size, f))\n",
    "\t\treturn vit_feat\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tvit_feat, \n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\t# print(\"---VIT FORWARD---\")\n",
    "\t\tprint(f\"{vit_feat.shape = }\")\n",
    "\t\tx = self.conv_1d_1(vit_feat)\n",
    "\t\tprint(f\"{x.shape = }\")\n",
    "\t\tx = self.layer_1_relu(x)\n",
    "\t\tprint(f\"{x.shape = }\")\n",
    "\t\tx = torch.squeeze(x, dim=2)\n",
    "\t\tprint(f\"{x.shape = }\")\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Translator(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes,\n",
    "\t\tH_input_size: int = 151296,\n",
    "\t\tH_output_size: int = 10,\n",
    "\t\tnum_layers: int = 1,\n",
    "\t\tdropout: int = 0,\n",
    "\t):\n",
    "\t\t# print(\"---GRU INIT---\")\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\t\tself.vocabulary_size = nb_classes\n",
    "\t\tself.layer_gru = nn.GRU(\n",
    "\t\t\tinput_size=self.hparams.H_input_size,\n",
    "\t\t\thidden_size=self.hparams.nb_classes,\n",
    "\t\t\tnum_layers=self.hparams.num_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=self.hparams.dropout\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.layer_1_dense = nn.Linear(self.hparams.nb_classes, int(self.hparams.nb_classes / 2))\n",
    "\t\tself.layer_2_dense = nn.Linear(int(self.hparams.nb_classes / 2), self.hparams.nb_classes)\n",
    "\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\t# self.layer_2_dense = nn.Linear(self.hparams.H_output_size, self.vocabulary_size)\n",
    "\t\tself.layer_leaky_relu = nn.LeakyReLU()\n",
    "\t\tself.softmax = nn.Softmax(dim=2) # <-- i/o dim=2\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tX, _ = self.layer_gru(X)\n",
    "\t\tX = self.layer_1_dense(X)\n",
    "\t\tX = self.layer_1_relu(X)\n",
    "\t\tX = self.layer_2_dense(X)\n",
    "\t\tX = self.softmax(X)\n",
    "\t\treturn X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSquareNet(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tbatch_size: int = 1,\n",
    "\t\tseq_size: int = 1,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t\th_in: int = 10,\n",
    "\t\tk_features: int = 64,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.image_feature_extractor = ViT_FeatureExtractor(\n",
    "\t\t\tnb_classes=self.hparams.nb_classes, \n",
    "\t\t\tbatch_size=self.hparams.batch_size, \n",
    "\t\t\tseq_size=self.hparams.seq_size,\n",
    "\t\t\tout_features=self.hparams.k_features\n",
    "\t\t)\n",
    "\t\tself.recurrent_translator = GRU_Translator(\n",
    "\t\t\tnb_classes = nb_classes,\n",
    "\t\t\tH_input_size=h_in,\n",
    "\t\t\tnum_layers=1,\n",
    "\t\t\tdropout=0\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\tprint(f\"IN: {x.shape=}\")\n",
    "\t\tb, s, k, f = x .size()\n",
    "\t\tx = torch.reshape(x, (b * s, k, f))\n",
    "\t\tprint(f\"VIT: {x.shape=}\")\n",
    "\n",
    "\t\tx = self.image_feature_extractor(x)\n",
    "\t\tprint(f\"FEAT: {x.shape=}\")\n",
    "\n",
    "\t\tb = self.hparams.batch_size\n",
    "\t\ts = self.hparams.seq_size\n",
    "\t\tf = self.hparams.k_features\n",
    "\n",
    "\t\tx = torch.flatten(x, start_dim=1)\n",
    "\t\tbs, _f = x.size()\n",
    "\t\t# IN: x.shape=torch.Size([2, 3, 197, 768])\n",
    "\t\t# VIT: x.shape=torch.Size([6, 197, 768])\n",
    "\t\t# FEAT: x.shape=torch.Size([6, 64])\n",
    "\t\tassert(f == _f)\n",
    "\t\tassert(b * s == bs)\n",
    "\t\tx = torch.reshape(x, (b, s, f))\n",
    "\t\tprint(f\"GRU: {x.shape=}\")\n",
    "\t\tx = self.recurrent_translator(x)\n",
    "\t\tprint(f\"Out: {x.shape=}\")\n",
    "\t\treturn x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "a.shape = torch.Size([3, 4])\n",
      "x.shape = torch.Size([3, 4])\n",
      "x = tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]]])\n",
      "x.shape = torch.Size([1, 3, 4])\n",
      "x = tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]]])\n",
      "x.shape = torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "b = 1\n",
    "s = 3\n",
    "f = 4\n",
    "\n",
    "a = torch.arange(0, b * s * f).reshape(b * s, f)\n",
    "x = a\n",
    "print(f\"{a = }\")\n",
    "print(f\"{a.shape = }\")\n",
    "\n",
    "print(f\"{x.shape = }\")\n",
    "x = torch.flatten(x, start_dim=1)\n",
    "bb, ff = x.size()\n",
    "x = torch.reshape(x, (bb // s, bb // b, ff))\n",
    "# x = torch.unsqueeze(x, dim=0)\n",
    "# x = torch.unsqueeze(x, dim=0)\n",
    "print(f\"{x = }\")\n",
    "print(f\"{x.shape = }\")\n",
    "print(f\"{x = }\")\n",
    "print(f\"{x.shape = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nb_classes=10\n",
    "seq_size = 3\n",
    "batch_size = 2\n",
    "learning_rate = 1e-4\n",
    "h_in = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size()=torch.Size([6, 3, 224, 224])\n",
      "y.size()=torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((batch_size * seq_size, 3, 224, 224))\n",
    "y = torch.randint(0, nb_classes, (batch_size, seq_size))\n",
    "\n",
    "print(f\"{x.size()=}\")\n",
    "print(f\"{y.size()=}\")\n",
    "# print(f\"{y=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_feat.shape = torch.Size([6, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# Models\n",
    "# vit_feat = model.image_feature_extractr.vit_extract_features(x)\n",
    "\n",
    "# dataset = SignedDataset(vit_feat, y)\n",
    "model = BaseSquareNet(batch_size=batch_size, seq_size=seq_size, nb_classes=nb_classes, h_in=h_in)\n",
    "vit_feat = model.image_feature_extractor.vit_extract_features(x)\n",
    "dataset = SignedDataset(vit_feat, y, seq_size)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size * seq_size)\n",
    "\t# lr = 10 ** (- e_lr / 10)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader, model, loss_fn, learning_rate, epochs=100):\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\tlosses = []\n",
    "\tidx = 0\n",
    "\tfor epoch in tqdm(range(epochs)):\n",
    "\t\tfor batch_idx, (X, y) in enumerate(train_loader):\n",
    "\t\t\t# print(idx)\n",
    "\t\t\tpred = model(X)\n",
    "\t\t\t# print(f\"{pred.shape=}\")\n",
    "\t\t\tpred = pred.permute(0, 2, 1)\n",
    "\t\t\tloss = loss_fn(pred, y)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# if epoch % 10 == 0:\n",
    "\t\t\t# \tprint(f'[{epoch = }] loss: {loss}')\n",
    "\t\t\tidx += 1\n",
    "\t\t\tlosses.append(float(loss.detach().numpy()))\n",
    "\treturn losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.X.shape = torch.Size([6, 197, 768])\n",
      "dataset.Y.shape = torch.Size([2, 3])\n",
      "2\n",
      "b_x.shape = torch.Size([3, 197, 768])\n",
      "b_y.shape = torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dataset.X.shape = }\")\n",
    "print(f\"{dataset.Y.shape = }\")\n",
    "print(f\"{len(dataset)}\")\n",
    "b_x, b_y = dataset[1]\n",
    "print(f\"{b_x.shape = }\")\n",
    "print(f\"{b_y.shape = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx = 0\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (X, y) in enumerate(dataloader):\n",
    "\tprint(f\"{batch_idx = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: x.shape=torch.Size([2, 3, 197, 768])\n",
      "VIT: x.shape=torch.Size([6, 197, 768])\n",
      "vit_feat.shape = torch.Size([6, 197, 768])\n",
      "x.shape = torch.Size([6, 64, 1])\n",
      "x.shape = torch.Size([6, 64, 1])\n",
      "x.shape = torch.Size([6, 64])\n",
      "FEAT: x.shape=torch.Size([6, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19107/3478784239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# model = GRU(input_size=h_in, hidden_size=5, output_size=nb_classes, num_layers=1, bias=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseSquareNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3e3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"For {lr = }, {min(losses) = }\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"{lr:e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19107/4152521309.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, loss_fn, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                         \u001b[0;31m# print(idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                         \u001b[0;31m# print(f\"{pred.shape=}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19107/2074289380.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GRU: {x.shape=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1152x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "torch.manual_seed(6)\n",
    "\n",
    "for lr in [5e-4]:\n",
    "\t# lr = 10 ** (- e_lr / 10)\n",
    "\t# model = GRU(input_size=h_in, hidden_size=5, output_size=nb_classes, num_layers=1, bias=0)\n",
    "\tmodel = BaseSquareNet(nb_classes=nb_classes, seq_size=seq_size, batch_size=batch_size, h_in=h_in)\n",
    "\tlosses = train(dataloader, model, loss_fn, learning_rate=lr, epochs=int(3e3))\n",
    "\tprint(f\"For {lr = }, {min(losses) = }\")\n",
    "\tplt.plot(losses, label=f\"{lr:e}\")\n",
    "leg = plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Hand2Text')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf687d64d53e3adc62f479116506dad1cadccbe1000f1194a9eb47c325d46594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
