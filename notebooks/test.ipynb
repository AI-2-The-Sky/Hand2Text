{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolmalin/miniconda/envs/Hand2Text/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "from nptyping import Float32, NDArray, Number, Shape, UInt\n",
    "from transformers import ViTModel\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "# from src.models.components.baseline.BaseSquareNet import BaseSquareNet\n",
    "# from src.models.components.vit_baseline import ViTBaselineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignedDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        # [n_video, nb_frames, 3, 320, 240]\n",
    "        self.Y = Y\n",
    "        # [n_video, nb_signes, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_FeatureExtractor(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: str = \"/usr/share/dict/words\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.vocabulary_size = len(np.array(open(corpus).read().splitlines()))\n",
    "        self.vocabulary_size = 1999\n",
    "\n",
    "        self.pretrained_vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.pretrained_vit.eval()\n",
    "\n",
    "        self.conv_1d_1 = torch.nn.Conv1d(\n",
    "            in_channels=197,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.layer_1_relu = nn.ReLU()\n",
    "        self.conv_1d_2 = torch.nn.Conv1d(\n",
    "            in_channels=64,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.layer_2_relu = nn.ReLU()\n",
    "\n",
    "    def forward(\n",
    "        self, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "    ) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\n",
    "        outputs = self.pretrained_vit(pixel_values=x)\n",
    "        vit_feat = outputs.last_hidden_state\n",
    "\n",
    "        x = self.conv_1d_1(vit_feat)\n",
    "        x = self.layer_1_relu(x)\n",
    "        x = self.conv_1d_2(x)\n",
    "        x = self.layer_2_relu(x)\n",
    "        x = torch.squeeze(x, dim=0)\n",
    "        print(f\"{x.shape= }\")\n",
    "        return x\n",
    "\n",
    "class ViT(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: str = \"/usr/share/dict/words\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.vocabulary_size = len(np.array(open(corpus).read().splitlines()))\n",
    "        self.vocabulary_size = 1999\n",
    "\n",
    "        self.pretrained_vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.pretrained_vit.eval()\n",
    "\n",
    "        self.conv_1d_1 = torch.nn.Conv1d(\n",
    "            in_channels=197,\n",
    "            out_channels=64,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.layer_1_relu = nn.ReLU()\n",
    "        self.conv_1d_2 = torch.nn.Conv1d(\n",
    "            in_channels=64,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "        self.layer_2_relu = nn.ReLU()\n",
    "        self.dense = nn.Linear(764, self.vocabulary_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(\n",
    "        self, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "    ) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\n",
    "        outputs = self.pretrained_vit(pixel_values=x)\n",
    "        vit_feat = outputs.last_hidden_state\n",
    "\n",
    "        x = self.conv_1d_1(vit_feat)\n",
    "        x = self.layer_1_relu(x)\n",
    "        x = self.conv_1d_2(x)\n",
    "        x = self.layer_2_relu(x)\n",
    "        # x = torch.squeeze(x, dim=0)\n",
    "        # print(f\"{x.shape= }\")\n",
    "        x = self.dense(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Translator(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        H_input_size: int = 764,\n",
    "        H_output_size: int = 100,\n",
    "        num_layers: int = 1,\n",
    "        dropout: int = 0,\n",
    "        corpus: str = \"/usr/share/dict/words\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # self.vocabulary_size = len(np.array(open(corpus).read().splitlines()))\n",
    "        self.vocabulary_size = 1999\n",
    "        self.layer_gru = nn.GRU(\n",
    "            input_size=self.hparams.H_input_size,\n",
    "            hidden_size=self.hparams.H_output_size,\n",
    "            num_layers=self.hparams.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "\n",
    "        self.layer_1_dense = nn.Linear(self.hparams.H_output_size, self.hparams.H_output_size)\n",
    "        self.layer_1_relu = nn.ReLU()\n",
    "        self.layer_2_dense = nn.Linear(self.hparams.H_output_size, self.vocabulary_size)\n",
    "        self.layer_2_relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        # print(f\"gru: {X.shape = }\")\n",
    "        X, hidden = self.layer_gru(X)\n",
    "        # print(f\"gru: {X.shape = }\")\n",
    "        X = self.layer_1_dense(X)\n",
    "        X = self.layer_1_relu(X)\n",
    "        # print(f\"gru: {X.shape = }\")\n",
    "        X = self.layer_2_dense(X)\n",
    "        X = self.layer_2_relu(X)\n",
    "        # print(f\"gru: {X.shape = }\")\n",
    "        X = self.softmax(X)\n",
    "        print(f\"{X.shape= }\")\n",
    "        # print(f\"gru: {X.shape = }\")\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSquareNet(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        corpus: str = \"/usr/share/dict/words\",\n",
    "        sequence_size: int = 16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # self.vocabulary_size = len(np.array(open(corpus).read().splitlines()))\n",
    "        self.vocabulary_size = 1999\n",
    "        # self.image_feature_extractr = ViT_FeatureExtractor(corpus)\n",
    "        self.image_feature_extractr = ViT(corpus)\n",
    "        self.recurrent_translator = GRU_Translator(\n",
    "            H_input_size=764,\n",
    "            H_output_size=100,\n",
    "            num_layers=1,\n",
    "            dropout=0,\n",
    "            corpus=corpus,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "    ) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "        # x_seq = []\n",
    "        # for i in range(self.hparams.sequence_size):\n",
    "        #     print(f\"{x.shape = }\")\n",
    "        #     b, f = x.shape\n",
    "        #     x = x.view((b, 1, f))\n",
    "        #     print(f\"{x.shape = }\")\n",
    "        #     x_seq.append(x)\n",
    "        # x_seq = torch.cat(x_seq, dim=1)\n",
    "        # print(f\"In: {x.shape = }\")\n",
    "        x = self.image_feature_extractr(x)\n",
    "        # print(f\"Vit: {x.shape = }\")\n",
    "\n",
    "        # b, f = x.shape\n",
    "        # x_seq = x.view(1, b, f)\n",
    "        # x = self.recurrent_translator(x_seq)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape= torch.Size([1, 3, 224, 244])\n",
      "y.shape= torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# x = np.random.rand(1, 3, 224, 224)\n",
    "# x = torch.tensor(x, dtype=torch.float)\n",
    "x = torch.rand((1, 3, 224, 244))\n",
    "y = torch.tensor([0])\n",
    "\n",
    "print(f\"{x.shape= }\")\n",
    "print(f\"{y.shape= }\")\n",
    "\n",
    "dataset = SignedDataset(x, y)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1)\n",
    "model = BaseSquareNet()\n",
    "\n",
    "learning_rate = 0.002\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, loss_fn, optmizer):\n",
    "    size = len(train_loader.dataset)\n",
    "    batches_l = len(train_loader)\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    while True:\n",
    "        for batch_idx, (X, y) in enumerate(train_loader):\n",
    "            pred = torch.squeeze(model(X), dim=0)\n",
    "            loss = loss_fn(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'loss: {loss}\\r', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input image size (224*244) doesn't match model (224*224).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=0'>1</a>\u001b[0m train(dataloader, model, loss_fn, optimizer)\n",
      "\u001b[1;32m/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, loss_fn, optmizer)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=6'>7</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=8'>9</a>\u001b[0m         pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(model(X), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=9'>10</a>\u001b[0m         loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=11'>12</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb Cell 8\u001b[0m in \u001b[0;36mBaseSquareNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=22'>23</a>\u001b[0m     \u001b[39mself\u001b[39m, x: NDArray[Shape[\u001b[39m\"\u001b[39m\u001b[39m* batch, 224, 224, 3\u001b[39m\u001b[39m\"\u001b[39m], Float32]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=23'>24</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDArray[Shape[\u001b[39m\"\u001b[39m\u001b[39m* batch, * vocab size\u001b[39m\u001b[39m\"\u001b[39m], Float32]:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=31'>32</a>\u001b[0m     \u001b[39m# x_seq = torch.cat(x_seq, dim=1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=32'>33</a>\u001b[0m     \u001b[39m# print(f\"In: {x.shape = }\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=33'>34</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_feature_extractr(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=34'>35</a>\u001b[0m     \u001b[39m# print(f\"Vit: {x.shape = }\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=35'>36</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=36'>37</a>\u001b[0m     \u001b[39m# b, f = x.shape\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=37'>38</a>\u001b[0m     \u001b[39m# x_seq = x.view(1, b, f)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=38'>39</a>\u001b[0m     \u001b[39m# x = self.recurrent_translator(x_seq)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb Cell 8\u001b[0m in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=69'>70</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=70'>71</a>\u001b[0m     \u001b[39mself\u001b[39m, x: NDArray[Shape[\u001b[39m\"\u001b[39m\u001b[39m* batch, 224, 224, 3\u001b[39m\u001b[39m\"\u001b[39m], Float32]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=71'>72</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NDArray[Shape[\u001b[39m\"\u001b[39m\u001b[39m* batch, * vocab size\u001b[39m\u001b[39m\"\u001b[39m], Float32]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=73'>74</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpretrained_vit(pixel_values\u001b[39m=\u001b[39;49mx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=74'>75</a>\u001b[0m     vit_feat \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dolmalin/Documents/work/42ai/Hand2Text/notebooks/test.ipynb#ch0000007?line=76'>77</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_1d_1(vit_feat)\n",
      "File \u001b[0;32m~/miniconda/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/Hand2Text/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:573\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 573\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    574\u001b[0m     pixel_values, bool_masked_pos\u001b[39m=\u001b[39;49mbool_masked_pos, interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding\n\u001b[1;32m    575\u001b[0m )\n\u001b[1;32m    577\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    578\u001b[0m     embedding_output,\n\u001b[1;32m    579\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    583\u001b[0m )\n\u001b[1;32m    584\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/Hand2Text/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:135\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     pixel_values: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    131\u001b[0m     bool_masked_pos: Optional[torch\u001b[39m.\u001b[39mBoolTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m     interpolate_pos_encoding: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    133\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    134\u001b[0m     batch_size, num_channels, height, width \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 135\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding)\n\u001b[1;32m    137\u001b[0m     batch_size, seq_len, _ \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39msize()\n\u001b[1;32m    138\u001b[0m     \u001b[39mif\u001b[39;00m bool_masked_pos \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/Hand2Text/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/Hand2Text/lib/python3.8/site-packages/transformers/models/vit/modeling_vit.py:188\u001b[0m, in \u001b[0;36mPatchEmbeddings.forward\u001b[0;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m interpolate_pos_encoding:\n\u001b[1;32m    187\u001b[0m     \u001b[39mif\u001b[39;00m height \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m width \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 188\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput image size (\u001b[39m\u001b[39m{\u001b[39;00mheight\u001b[39m}\u001b[39;00m\u001b[39m*\u001b[39m\u001b[39m{\u001b[39;00mwidth\u001b[39m}\u001b[39;00m\u001b[39m) doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m*\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_size[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection(pixel_values)\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: Input image size (224*244) doesn't match model (224*224)."
     ]
    }
   ],
   "source": [
    "train(dataloader, model, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Hand2Text')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65fb5dd8ae324f83aa5144c1c5dc9ef961af4827b21b9b1b28ba5f6a989edf83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
