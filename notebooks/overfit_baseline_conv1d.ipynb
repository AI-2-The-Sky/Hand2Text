{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "import numpy as np\n",
    "from nptyping import Float32, NDArray, Number, Shape, UInt\n",
    "from transformers import ViTModel\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignedDataset(Dataset):\n",
    "    def __init__(self, X, Y, seq_size):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.seq_size = seq_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x_idx_a = seq_size *  i\n",
    "        x_idx_z = seq_size * (i+1)\n",
    "        batch_X = self.X[x_idx_a:x_idx_z]\n",
    "\t\t\n",
    "        batch_Y = self.Y[i]\n",
    "        return batch_X, batch_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_FeatureExtractor(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t\tbatch_size: int = 2,\n",
    "\t\tseq_size: int = 2,\n",
    "\t\tout_features: int = 64,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.seq_size = seq_size\n",
    "\t\tself.pretrained_vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\t\tself.pretrained_vit.eval()\n",
    "\t\tself.conv_1d_1 = torch.nn.Conv1d(\n",
    "\t\t\tin_channels=197,\n",
    "\t\t\tout_channels=self.hparams.out_features,\n",
    "\t\t\tkernel_size=768,\n",
    "\t\t)\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\t# self.conv_1d_2 = torch.nn.Conv1d(\n",
    "\t\t# \tin_channels=64,\n",
    "\t\t# \tout_channels=nb_classes, # <-- i/o 1\n",
    "\t\t# \tkernel_size=3,\n",
    "\t\t# )\n",
    "\t\t# self.layer_2_relu = nn.ReLU()\n",
    "\n",
    "\tdef vit_extract_features(self, x):\n",
    "\t\t# print(\"---VIT EXTRACT FEATURES---\")\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = self.pretrained_vit(pixel_values=x)\n",
    "\t\t\tvit_feat = outputs.last_hidden_state\n",
    "\t\t\t# print(f\"{vit_feat.shape = }\")\n",
    "\t\t\t# vit_feat = torch.flatten(vit_feat, start_dim=1)\n",
    "\t\t\t# b, f = vit_feat.size()\n",
    "\t\t\t# vit_feat = torch.reshape(vit_feat, (self.batch_size, self.seq_size, f))\n",
    "\t\treturn vit_feat\n",
    "\t\n",
    "\tdef forward(\n",
    "\t\tself,\n",
    "\t\tvit_feat, \n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\t# print(\"---VIT FORWARD---\")\n",
    "\t\t# print(f\"{vit_feat.shape = }\")\n",
    "\t\tx = self.conv_1d_1(vit_feat)\n",
    "\t\t# print(f\"{x.shape = }\")\n",
    "\t\tx = self.layer_1_relu(x)\n",
    "\t\t# print(f\"{x.shape = }\")\n",
    "\t\tx = torch.squeeze(x, dim=2)\n",
    "\t\t# print(f\"{x.shape = }\")\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_Translator(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tnb_classes,\n",
    "\t\tH_input_size: int = 151296,\n",
    "\t\tH_output_size: int = 10,\n",
    "\t\tnum_layers: int = 1,\n",
    "\t\tdropout: int = 0,\n",
    "\t):\n",
    "\t\t# print(\"---GRU INIT---\")\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\t\tself.vocabulary_size = nb_classes\n",
    "\t\tself.layer_gru = nn.GRU(\n",
    "\t\t\tinput_size=self.hparams.H_input_size,\n",
    "\t\t\thidden_size=self.hparams.nb_classes,\n",
    "\t\t\tnum_layers=self.hparams.num_layers,\n",
    "\t\t\tbatch_first=True,\n",
    "\t\t\tdropout=self.hparams.dropout\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.layer_1_dense = nn.Linear(self.hparams.nb_classes, self.hparams.nb_classes)\n",
    "\t\t# self.layer_1_dense = nn.Linear(self.hparams.nb_classes, int(self.hparams.nb_classes / 2))\n",
    "\t\t# self.layer_2_dense = nn.Linear(int(self.hparams.nb_classes / 2), self.hparams.nb_classes)\n",
    "\n",
    "\t\tself.layer_1_relu = nn.ReLU()\n",
    "\t\t# self.layer_2_dense = nn.Linear(self.hparams.H_output_size, self.vocabulary_size)\n",
    "\t\tself.layer_leaky_relu = nn.LeakyReLU()\n",
    "\t\tself.softmax = nn.Softmax(dim=2) # <-- i/o dim=2\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\tX, _ = self.layer_gru(X)\n",
    "\t\tX = self.layer_1_dense(X)\n",
    "\t\t# X = self.layer_1_relu(X)\n",
    "\t\t# X = self.layer_2_dense(X)\n",
    "\t\tX = self.softmax(X)\n",
    "\t\treturn X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSquareNet(pl.LightningModule):\n",
    "\tdef __init__(\n",
    "\t\tself,\n",
    "\t\tbatch_size: int = 1,\n",
    "\t\tseq_size: int = 1,\n",
    "\t\tnb_classes: int = 10,\n",
    "\t\th_in: int = 10,\n",
    "\t\tk_features: int = 64,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\n",
    "\t\tself.image_feature_extractor = ViT_FeatureExtractor(\n",
    "\t\t\tnb_classes=self.hparams.nb_classes, \n",
    "\t\t\tbatch_size=self.hparams.batch_size, \n",
    "\t\t\tseq_size=self.hparams.seq_size,\n",
    "\t\t\tout_features=self.hparams.k_features\n",
    "\t\t)\n",
    "\t\tself.recurrent_translator = GRU_Translator(\n",
    "\t\t\tnb_classes = nb_classes,\n",
    "\t\t\tH_input_size=h_in,\n",
    "\t\t\tnum_layers=1,\n",
    "\t\t\tdropout=0\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(\n",
    "\t\tself, x: NDArray[Shape[\"* batch, 224, 224, 3\"], Float32]\n",
    "\t) -> NDArray[Shape[\"* batch, * vocab size\"], Float32]:\n",
    "\t\t# print(f\"IN: {x.shape=}\")\n",
    "\t\tb, s, k, f = x .size()\n",
    "\t\tx = x.view(b * s, k, f)\n",
    "\t\t# print(f\"VIT: {x.shape=}\")\n",
    "\n",
    "\t\tx = self.image_feature_extractor(x)\n",
    "\t\t# print(f\"FEAT: {x.shape=}\")\n",
    "\n",
    "\t\tb = self.hparams.batch_size\n",
    "\t\ts = self.hparams.seq_size\n",
    "\t\tf = self.hparams.k_features\n",
    "\n",
    "\t\tx = torch.flatten(x, start_dim=1)\n",
    "\t\tbs, _f = x.size()\n",
    "\t\t# IN: x.shape=torch.Size([2, 3, 197, 768])\n",
    "\t\t# VIT: x.shape=torch.Size([6, 197, 768])\n",
    "\t\t# FEAT: x.shape=torch.Size([6, 64])\n",
    "\t\tassert(f == _f)\n",
    "\t\tassert(b * s == bs)\n",
    "\t\tx = x.view(b, s, f)\n",
    "\t\t# print(f\"GRU: {x.shape=}\")\n",
    "\t\tx = self.recurrent_translator(x)\n",
    "\t\t# print(f\"Out: {x.shape=}\")\n",
    "\t\treturn x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "a.shape = torch.Size([3, 4])\n",
      "x.shape = torch.Size([3, 4])\n",
      "x = tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]]])\n",
      "x.shape = torch.Size([1, 3, 4])\n",
      "x = tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]]])\n",
      "x.shape = torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "b = 1\n",
    "s = 3\n",
    "f = 4\n",
    "\n",
    "a = torch.arange(0, b * s * f).reshape(b * s, f)\n",
    "x = a\n",
    "print(f\"{a = }\")\n",
    "print(f\"{a.shape = }\")\n",
    "\n",
    "print(f\"{x.shape = }\")\n",
    "x = torch.flatten(x, start_dim=1)\n",
    "bb, ff = x.size()\n",
    "x = torch.reshape(x, (bb // s, bb // b, ff))\n",
    "# x = torch.unsqueeze(x, dim=0)\n",
    "# x = torch.unsqueeze(x, dim=0)\n",
    "print(f\"{x = }\")\n",
    "print(f\"{x.shape = }\")\n",
    "print(f\"{x = }\")\n",
    "print(f\"{x.shape = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nb_classes=10\n",
    "seq_size = 3\n",
    "batch_size = 2\n",
    "learning_rate = 1e-4\n",
    "h_in = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size()=torch.Size([6, 3, 224, 224])\n",
      "y.size()=torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((batch_size * seq_size, 3, 224, 224))\n",
    "y = torch.randint(0, nb_classes, (batch_size, seq_size))\n",
    "\n",
    "print(f\"{x.size()=}\")\n",
    "print(f\"{y.size()=}\")\n",
    "# print(f\"{y=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "# Models\n",
    "# vit_feat = model.image_feature_extractr.vit_extract_features(x)\n",
    "\n",
    "# dataset = SignedDataset(vit_feat, y)\n",
    "model = BaseSquareNet(batch_size=batch_size, seq_size=seq_size, nb_classes=nb_classes, h_in=h_in)\n",
    "vit_feat = model.image_feature_extractor.vit_extract_features(x)\n",
    "dataset = SignedDataset(vit_feat, y, seq_size)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size * seq_size)\n",
    "\t# lr = 10 ** (- e_lr / 10)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(train_loader, model, loss_fn, learning_rate, epochs=100):\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\tlosses = []\n",
    "\tidx = 0\n",
    "\tpbar = tqdm(range(epochs))\n",
    "\tfor epoch in pbar:\n",
    "\t\tfor batch_idx, (X, y) in enumerate(train_loader):\n",
    "\t\t\t# print(idx)\n",
    "\t\t\tpred = model(X)\n",
    "\t\t\t# print(f\"{pred.shape=}\")\n",
    "\t\t\tpred = pred.permute(0, 2, 1)\n",
    "\t\t\tloss = loss_fn(pred, y)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# if epoch % 10 == 0:\n",
    "\t\t\t# \tprint(f'[{epoch = }] loss: {loss}')\n",
    "\t\t\tidx += 1\n",
    "\t\t\tlosses.append(float(loss.detach().numpy()))\n",
    "\t\tpbar.set_description(desc=f\"{losses[-1]}\", refresh=True)\n",
    "\treturn losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.X.shape = torch.Size([6, 197, 768])\n",
      "dataset.Y.shape = torch.Size([2, 3])\n",
      "2\n",
      "b_x.shape = torch.Size([3, 197, 768])\n",
      "b_y.shape = torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{dataset.X.shape = }\")\n",
    "print(f\"{dataset.Y.shape = }\")\n",
    "print(f\"{len(dataset)}\")\n",
    "b_x, b_y = dataset[1]\n",
    "print(f\"{b_x.shape = }\")\n",
    "print(f\"{b_y.shape = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx = 0\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (X, y) in enumerate(dataloader):\n",
    "\tprint(f\"{batch_idx = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.1278903484344482:  34%|███▍      | 343/1000 [00:48<01:37,  6.75it/s]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "torch.manual_seed(6)\n",
    "\n",
    "for lr in [3e-4, 5e-4, 7e-4]:\n",
    "\t# lr = 10 ** (- e_lr / 10)\n",
    "\t# model = GRU(input_size=h_in, hidden_size=5, output_size=nb_classes, num_layers=1, bias=0)\n",
    "\tmodel = BaseSquareNet(nb_classes=nb_classes, seq_size=seq_size, batch_size=batch_size, h_in=h_in)\n",
    "\tlosses = train(dataloader, model, loss_fn, learning_rate=lr, epochs=int(1e3))\n",
    "\tprint(f\"For {lr = }, {min(losses) = }\")\n",
    "\tplt.plot(losses, label=f\"{lr:e}\")\n",
    "leg = plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Hand2Text')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf687d64d53e3adc62f479116506dad1cadccbe1000f1194a9eb47c325d46594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
